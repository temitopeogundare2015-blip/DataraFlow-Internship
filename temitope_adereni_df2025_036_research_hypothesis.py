# -*- coding: utf-8 -*-
"""Temitope Adereni - DF2025-036 - Research Hypothesis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15W4KMK_5aatcfMZpDqLyis2-LkuB-asx

# **Predictive Modeling: Utilizing Machine Learning to Forecast Diabetes Hospital Re-admissions**

Hospital readmissions are a major and ongoing concern for healthcare systems worldwide and serve as a complex indicator of care quality while contributing billions in preventable costs each year. The likelihood of readmission among patients with diabetes mellitus is particularly high and this reflects the difficulties of managing multiple comorbid conditions and ensuring effective continuity of care. Despite substantial progress, many existing predictive models still face limitations such as poor generalizability, challenges in addressing class imbalance, and limited clinical interpretability. This study proposes a systematic and reproducible framework for predicting 30-day hospital readmissions using the UCI Diabetes 130-US Hospitals dataset. It involves a comparative evaluation of Logistic Regression and Random Forest classifiers, emphasizing Recall optimization through a structured assessment of class imbalance correction methods. Explainable AI (XAI) techniques, particularly SHAP values, will be applied to enhance model transparency and results will be visualized in Power BI dashboards to support data-driven decision-making by clinical and operational teams. The findings are expected to highlight the balance between model interpretability and predictive performance, and offer practical guidance for developing reliable, usable, and transparent prediction models in healthcare.

## **Hypotheses (Hs):**

**H1:** Random forest classifiers will outperform logistic regression in terms of discrimination (AUC, F1-score) on the diabetes readmission dataset.

**H2:** Incorporating utilisation-related features (for example, prior admissions and length of stay) will significantly improve prediction accuracy compared to models trained on demographic variables alone.

**H3:** Class imbalance correction methods will improve recall of readmitted cases while maintaining acceptable calibration.
"""



"""##ðŸ’¡ **Setup and Data Acquisition**

In this section, we prepare the environment and load the dataset.

ðŸ“š Import Libraries

We use pandas and numpy for data handling, seaborn and matplotlib for visualization, and sklearn for modeling.


"""

# Import python libraries
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# Import dataset
Diabetic_data = pd.read_csv('diabetic_data.csv')
Diabetic_data

# Drop irrelevant column
Diabetic_data = Diabetic_data.drop(columns=['encounter_id', 'patient_nbr', 'weight', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'payer_code', 'medical_specialty'])

# Download the clean dataset after dropping irrelevant colums
Diabetic_data.to_csv('cleaned_Diabetic_data.csv', index=False)

# Importing the clean dataset
Diabetic_data = pd.read_csv('cleaned_Diabetic_data.csv')
Diabetic_data

"""This approach is a simple preprocessing workflow to prepare the diabetes dataset for modelling. First, required Python libraries were imported and warnings are suppressed to keep the output clean. The raw dataset is then loaded into a dataframe. I later removed columns considered irrelevant or noisy for prediction, such as administrative identifiers and highly sparse variables that add little clinical signal. For instance encounter_id and patient_nbr are numbers pertaining to Dropping these features reduces dimensionality, improves model generalisation and limits the risk of overfitting. The cleaned dataset is saved as a new CSV file to preserve a reproducible preprocessing checkpoint. Finally, the cleaned dataset is reloaded to confirm the structure and ensure subsequent analysis uses the curated version."""

# Shape of the dataset
Diabetic_data.shape

# Dataset types
Diabetic_data.dtypes

# Converts every '?' in the dataset into a true missing value that pandas recognizes
Diabetic_data.replace('?', pd.NA, inplace=True)

#To show missing value in the dataset
print('\nMissing Values Count:')
Diabetic_data.isnull().sum()

# Extract independent variables (all columns except the last one)
X = Diabetic_data.iloc[:, :-1].values
X

# Extract dependent variables
Y = Diabetic_data.iloc[:, -1].values
Y

# Replace '?' with np.nan
Diabetic_data.replace("?", np.nan, inplace=True)

# Force race column to string (removes pd.NA ambiguity)
Diabetic_data['race'] = Diabetic_data['race'].astype(str)

# Convert "nan" string back to np.nan
Diabetic_data['race'].replace("nan", np.nan, inplace=True)

# Extract features
X = Diabetic_data.iloc[:, :-1].values

# Impute race column (mode)
imputer = SimpleImputer(strategy='most_frequent')
X[:, 0] = imputer.fit_transform(X[:, 0].reshape(-1, 1)).flatten()

X

# Handle missing values in the diag_1, diag_2 and diag_3 columns

from sklearn.impute import SimpleImputer

diag_cols = ['diag_1', 'diag_2', 'diag_3']

# Step 1: Replace '?' with np.nan
Diabetic_data[diag_cols] = Diabetic_data[diag_cols].replace('?', np.nan)

# Step 2: Convert ALL values to numeric (forces pd.NA â†’ np.nan)
Diabetic_data[diag_cols] = Diabetic_data[diag_cols].apply(pd.to_numeric, errors='coerce')

# Step 3: Impute with 0
imputer = SimpleImputer(strategy='constant', fill_value=0)
Diabetic_data[diag_cols] = imputer.fit_transform(Diabetic_data[diag_cols])

Diabetic_data

lab_cols = ['max_glu_serum', 'A1Cresult']
Diabetic_data[lab_cols] = Diabetic_data[lab_cols].fillna("missing")

#To show missing value in the dataset
print('\nMissing Values Count:')
Diabetic_data.isnull().sum()



"""## **Proving Hypothesis 1:**
Random forest classifiers will outperform logistic regression in terms of discrimination (AUC, F1-score) on the diabetes readmission dataset.

"""

X = Diabetic_data.iloc[:, :-1]
Y = Diabetic_data.iloc[:, -1]

X = pd.get_dummies(X, drop_first=True)
Y = np.where(Y == "<30", 1, 0)

# Splitting the dataset into train and test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)

# Train Logistic Regression
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, Y_train)

lr_pred = lr.predict(X_test)
lr_prob = lr.predict_proba(X_test)[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
fpr, tpr, _ = roc_curve(Y_test, lr_prob)
auc = roc_auc_score(Y_test, lr_prob)

plt.figure()
plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {auc:.3f})")
plt.plot([0,1], [0,1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(Y_test, lr_pred)

plt.figure()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Train Random Forest

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=300, random_state=0)
rf.fit(X_train, Y_train)

rf_pred = rf.predict(X_test)
rf_prob = rf.predict_proba(X_test)[:, 1]

# ROC values
fpr_lr, tpr_lr, _ = roc_curve(Y_test, lr_prob)
fpr_rf, tpr_rf, _ = roc_curve(Y_test, rf_prob)

plt.figure()
plt.plot(fpr_lr, tpr_lr, label="Logistic Regression")
plt.plot(fpr_rf, tpr_rf, label="Random Forest")
plt.plot([0,1], [0,1], linestyle="--")

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()

cm = confusion_matrix(Y_test, rf_pred)

plt.figure()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion Matrix")
plt.show()

# Evaluate both models

from sklearn.metrics import roc_auc_score, f1_score

lr_auc = roc_auc_score(Y_test, lr_prob)
rf_auc = roc_auc_score(Y_test, rf_prob)

lr_f1 = f1_score(Y_test, lr_pred)
rf_f1 = f1_score(Y_test, rf_pred)

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest"],
    "AUC": [lr_auc, rf_auc],
    "F1 Score": [lr_f1, rf_f1]
})

results

results.round(3)

"""Logistic regression achieved higher discrimination than the random forest model, with an AUC of 0.624 compared to 0.609 and a higher F1-score (0.028 vs 0.012). These results indicate that logistic regression provided superior classification performance on the diabetes readmission dataset. Consequently, the hypothesis that random forest classifiers would outperform logistic regression is not supported by the experimental evidence.

Contrary to expectations, logistic regression demonstrated superior discrimination compared to random forest. However, both models showed weak predictive performance, likely due to severe class imbalance. This highlights the importance of imbalance correction strategies explored in subsequent experiments.

#### **Evaluation of Hypothesis H1**

Hypothesis H1 proposed that random forest classifiers would outperform logistic regression in discriminating 30-day hospital readmissions. However, experimental results did not support this assumption. Logistic regression achieved a higher area under the ROC curve (AUC = 0.624) compared to the random forest model (AUC = 0.609), and also demonstrated a superior F1-score (0.028 vs 0.012). Although the performance difference is modest, it consistently favours logistic regression across both discrimination metrics. These findings indicate that the ensemble model did not provide the expected improvement over a simpler linear classifier on this dataset. Therefore, Hypothesis H1 is rejected. Importantly, both models exhibited weak overall performance, suggesting that severe class imbalance and limited signal in the predictors constrained predictive accuracy, motivating further investigation using imbalance correction strategies.

## **Proving Hypothesis 2:**

Incorporating utilisation-related features (for example, prior admissions and length of stay) will significantly improve prediction accuracy compared to models trained on demographic variables alone.

Hypothesis 2 involves experimental comparison using two controlled experiments.

Model A â†’ demographics only
Model B â†’ demographics + utilisation
Compare accuracy

If Model B performs better â†’ H2 supported.

> Add blockquote
"""

demo_cols = ['race', 'gender', 'age']

util_cols = [
    'time_in_hospital',
    'number_inpatient',
    'number_outpatient',
    'number_emergency'
]

# Create two datasets
X_demo = pd.get_dummies(Diabetic_data[demo_cols], drop_first=True)
X_util = pd.get_dummies(Diabetic_data[demo_cols + util_cols], drop_first=True)

Y = np.where(Diabetic_data['readmitted'] == '<30', 1, 0)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Spliting the new dataset
Xd_train, Xd_test, yd_train, yd_test = train_test_split(X_demo, Y, test_size=0.2, stratify=Y, random_state=42)

Xu_train, Xu_test, yu_train, yu_test = train_test_split(X_util, Y, test_size=0.2, stratify=Y, random_state=42)

# Model A: demographics only
model_demo = LogisticRegression(max_iter=1000)
model_demo.fit(Xd_train, yd_train)
pred_demo = model_demo.predict(Xd_test)

# Model B: demographics + utilisation
model_util = LogisticRegression(max_iter=1000)
model_util.fit(Xu_train, yu_train)
pred_util = model_util.predict(Xu_test)

# Comparing accuracy
acc_demo = accuracy_score(yd_test, pred_demo)
acc_util = accuracy_score(yu_test, pred_util)

print("Demographics accuracy:", acc_demo)
print("Demo + Utilisation accuracy:", acc_util)

"""### **Interpretation**

Incorporating utilisation-related variables did not improve classification accuracy. However, identical accuracy scores likely reflect severe class imbalance, where the model achieves high accuracy by predicting the majority class. Therefore, accuracy alone is insufficient to evaluate the contribution of utilisation features. Additional metrics focusing on minority-class detection are required.

### **Redoing the comparison using: AUC_demo vs AUC_util, F1_demo vs F1_util and Recall_demo vs Recall_util**
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import precision_recall_curve, roc_auc_score, average_precision_score, precision_score, recall_score, f1_score, confusion_matrix

# (Safety) keep only columns that exist
demo_cols = [c for c in demo_cols if c in Diabetic_data.columns]
util_cols = [c for c in util_cols if c in Diabetic_data.columns]

# -----------------------------
# 3) Train/Val/Test split (same split for both models)
# -----------------------------
X_all = Diabetic_data.drop(columns=["readmitted"], errors="ignore")

X_train, X_temp, Y_train, Y_temp = train_test_split(
    X_all, Y, test_size=0.30, stratify=Y, random_state=42
)
X_val, X_test, Y_val, Y_test = train_test_split(
    X_temp, Y_temp, test_size=0.50, stratify=Y_temp, random_state=42
)

# Helper: preprocessing + model
# -----------------------------
def build_lr_pipeline(feature_cols, class_weight_param=None):
    X_sub = X_train[feature_cols]

    num_cols = X_sub.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in feature_cols if c not in num_cols]

    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median"))
    ])

    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", num_pipe, num_cols),
            ("cat", cat_pipe, cat_cols)
        ],
        remainder="drop"
    )

    model = LogisticRegression(
        max_iter=2000,
        class_weight=class_weight_param, # Use the passed parameter
        random_state=42
    )

    pipe = Pipeline(steps=[
        ("prep", preprocessor),
        ("clf", model)
    ])

    return pipe

def tune_threshold_max_f1(Y_true, Y_prob):
    prec, rec, thr = precision_recall_curve(Y_true, Y_prob)
    f1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)
    best = np.argmax(f1s)
    return float(thr[best])

def evaluate(Y_true, Y_prob, threshold):
    Y_pred = (Y_prob >= threshold).astype(int)
    return {
        "AUC": roc_auc_score(Y_true, Y_prob),
        "PR_AUC": average_precision_score(Y_true, Y_prob),
        "Precision": precision_score(Y_true, Y_pred, zero_division=0),
        "Recall": recall_score(Y_true, Y_pred, zero_division=0),
        "F1": f1_score(Y_true, Y_pred, zero_division=0),
        "ConfusionMatrix(TN,FP,FN,TP)": confusion_matrix(Y_true, Y_pred).ravel().tolist(),
        "Threshold": threshold
    }

# Run Demographics-only model
# -----------------------------
pipe_demo = build_lr_pipeline(demo_cols)
pipe_demo.fit(X_train[demo_cols], Y_train)

demo_val_prob = pipe_demo.predict_proba(X_val[demo_cols])[:, 1]
thr_demo = tune_threshold_max_f1(Y_val, demo_val_prob)

demo_test_prob = pipe_demo.predict_proba(X_test[demo_cols])[:, 1]
metrics_demo = evaluate(Y_test, demo_test_prob, thr_demo)

# Run Demographics + Utilisation model
# -------------------------------------
pipe_util = build_lr_pipeline(demo_cols + util_cols)
pipe_util.fit(X_train[demo_cols + util_cols], Y_train)

util_val_prob = pipe_util.predict_proba(X_val[demo_cols + util_cols])[:, 1]
thr_util = tune_threshold_max_f1(Y_val, util_val_prob)

util_test_prob = pipe_util.predict_proba(X_test[demo_cols + util_cols])[:, 1]
metrics_util = evaluate(Y_test, util_test_prob, thr_util)

# Build comparison table
comparison_table = pd.DataFrame({
    'Model': ['Demographics Only', 'Demographics + Utilisation'],
    'AUC': [metrics_demo['AUC'], metrics_util['AUC']],
    'PR_AUC': [metrics_demo['PR_AUC'], metrics_util['PR_AUC']],
    'Precision': [metrics_demo['Precision'], metrics_util['Precision']],
    'Recall': [metrics_demo['Recall'], metrics_util['Recall']],
    'F1': [metrics_demo['F1'], metrics_util['F1']]
})

print(comparison_table.round(3))

# ROC curve
# Probabilities from the two models
demo_prob = demo_test_prob
util_prob = util_test_prob

# ROC values
fpr_demo, tpr_demo, _ = roc_curve(y_test, demo_prob)
fpr_util, tpr_util, _ = roc_curve(y_test, util_prob)

auc_demo = roc_auc_score(y_test, demo_prob)
auc_util = roc_auc_score(y_test, util_prob)

plt.figure(figsize=(6,6))

plt.plot(fpr_demo, tpr_demo,
         label=f"Demographics Only (AUC = {auc_demo:.3f})")

plt.plot(fpr_util, tpr_util,
         label=f"Demo + Utilisation (AUC = {auc_util:.3f})")

plt.plot([0,1], [0,1], linestyle="--", color="gray")

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison (H2)")
plt.legend()
plt.grid(True)

plt.show()

"""###**Evaluation of Hypothesis H2**

Hypothesis H2 proposed that incorporating utilisation-related features would improve predictive performance compared to models trained on demographic variables alone. The comparison demonstrates a clear improvement when utilisation variables were included. The model trained on demographics only achieved an AUC of 0.529 and an F1-score of 0.205, indicating weak discrimination and limited balance between precision and recall. After adding utilisation features, discrimination increased substantially (AUC = 0.629), accompanied by higher precision (0.177 vs 0.119) and an improved F1-score (0.243 vs 0.205). Although recall decreased (0.388 vs 0.765), the overall balance of classification performance improved, as reflected in the higher F1 and PR-AUC values. This suggests that utilisation features provide meaningful predictive signal and enhance the modelâ€™s ability to identify readmissions with greater reliability. Therefore, the experimental evidence supports Hypothesis H2.

## **Proving Hypothesis 3:**
Class imbalance correction methods will improve recall of readmitted cases while maintaining acceptable calibration.

To prove this hypothesis, we must show that;
*   imbalance correction â†’ higher recall
*   calibration is still acceptable

Therefore, we compare Model A (no imbalance handling) and Model B (with imbalance handling (SMOTE or class weights)) and then measure recall, AUC and calibration.
"""

from sklearn.linear_model import LogisticRegression

# Baseline model (Model A) (no correction)

# Get all feature columns for the base model
all_feature_cols = Diabetic_data.drop(columns=["readmitted"], errors="ignore").columns.tolist()

# Build and fit the base model using the pipeline, which handles preprocessing
base_pipe = build_lr_pipeline(all_feature_cols)
base_pipe.fit(X_train[all_feature_cols], Y_train)

# Predict probabilities and classes using the fitted pipeline
base_prob = base_pipe.predict_proba(X_test[all_feature_cols])[:, 1]
base_pred = base_pipe.predict(X_test[all_feature_cols])

# Evaluate the base model (no imbalance handling)
base_metrics = evaluate(Y_test, base_prob, tune_threshold_max_f1(Y_val, base_pipe.predict_proba(X_val[all_feature_cols])[:, 1]))
print("Base Model Metrics (no imbalance handling):")
print(pd.Series(base_metrics).round(3))

# Imbalance corrected model (Model B)

all_feature_cols = Diabetic_data.drop(columns=["readmitted"], errors="ignore").columns.tolist()

balanced_pipe = build_lr_pipeline(all_feature_cols, class_weight_param='balanced')
balanced_pipe.fit(X_train[all_feature_cols], Y_train)

bal_prob = balanced_pipe.predict_proba(X_test[all_feature_cols])[:, 1]
bal_pred = balanced_pipe.predict(X_test[all_feature_cols])

# Compare recall + AUC

from sklearn.metrics import recall_score, roc_auc_score

base_recall = recall_score(Y_test, base_pred)
bal_recall = recall_score(Y_test, bal_pred)

base_auc = roc_auc_score(Y_test, base_prob)
bal_auc = roc_auc_score(Y_test, bal_prob)

print("Baseline recall:", base_recall)
print("Balanced recall:", bal_recall)

print("Baseline AUC:", base_auc)
print("Balanced AUC:", bal_auc)

# Calibration check

from sklearn.calibration import calibration_curve

prob_true_base, prob_pred_base = calibration_curve(Y_test, base_prob, n_bins=10)
prob_true_bal, prob_pred_bal = calibration_curve(Y_test, bal_prob, n_bins=10)

plt.plot(prob_pred_base, prob_true_base, label="Baseline")
plt.plot(prob_pred_bal, prob_true_bal, label="Balanced")
plt.plot([0,1],[0,1],'--')

plt.xlabel("Predicted probability")
plt.ylabel("True probability")
plt.title("Calibration Curve")
plt.legend()
plt.show()

"""Class weighting did not alter recall or AUC, suggesting the baseline model was insensitive to class imbalance. This indicates that imbalance correction alone was insufficient to change decision behaviour under the default classification threshold.

**USING SMOTE**
"""

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)

# One-hot encode X_train before applying SMOTE
X_train_encoded = pd.get_dummies(X_train)

X_train_sm, Y_train_sm = smote.fit_resample(X_train_encoded, Y_train)

print("Before SMOTE:", np.bincount(Y_train))
print("After SMOTE:", np.bincount(Y_train_sm))

# Training SMOTE model

# Prepare X_test and X_val for the SMOTE model
# This involves one-hot encoding and aligning columns with X_train_sm

X_test_encoded_for_smote = pd.get_dummies(X_test)
X_val_encoded_for_smote = pd.get_dummies(X_val)

# Align columns to match X_train_sm (which was derived from X_train_encoded)
train_cols_smote = X_train_sm.columns

X_test_encoded_for_smote = X_test_encoded_for_smote.reindex(columns=train_cols_smote, fill_value=0)
X_val_encoded_for_smote = X_val_encoded_for_smote.reindex(columns=train_cols_smote, fill_value=0)

# Train Logistic Regression on SMOTE data
smote_lr = LogisticRegression(max_iter=2000, random_state=42)
smote_lr.fit(X_train_sm, Y_train_sm)

smote_prob = smote_lr.predict_proba(X_test_encoded_for_smote)[:, 1]
smote_pred = smote_lr.predict(X_test_encoded_for_smote)

# Evaluate SMOTE model
smote_metrics = evaluate(Y_test, smote_prob, tune_threshold_max_f1(Y_val, smote_lr.predict_proba(X_val_encoded_for_smote)[:, 1]))
print("SMOTE Model Metrics:")
print(pd.Series(smote_metrics).round(3))

# Compare baseline vs SMOTE

print("Baseline recall:", base_recall)
print("SMOTE recall:", recall_score(Y_test, smote_pred))
print("\nBaseline AUC:", base_auc)
print("SMOTE AUC:", smote_metrics['AUC'])

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Calculate ROC curve and AUC for Baseline model
fpr_base, tpr_base, _ = roc_curve(Y_test, base_prob)
auc_base = roc_auc_score(Y_test, base_prob)

# Calculate ROC curve and AUC for SMOTE model
fpr_smote, tpr_smote, _ = roc_curve(Y_test, smote_prob)
auc_smote = roc_auc_score(Y_test, smote_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr_base, tpr_base, label=f'Baseline Model (AUC = {auc_base:.3f})')
plt.plot(fpr_smote, tpr_smote, label=f'SMOTE Model (AUC = {auc_smote:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier') # Random classifier line

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison: Baseline vs. SMOTE')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""## **Evaluation of Hypothesis H3**

The application of SMOTE-based class imbalance correction did not improve model performance. Recall decreased from 0.495 in the baseline model to 0.423 after oversampling, while AUC also declined from 0.628 to 0.606. The confusion matrix indicates a substantial increase in false positives without a corresponding improvement in true positive detection. These findings suggest that synthetic oversampling introduced noise rather than enhancing minority-class representation. In high-dimensional clinical datasets, SMOTE may generate unrealistic synthetic cases that degrade discrimination rather than improve it. Consequently, the experimental evidence does not support Hypothesis H3. Imbalance correction alone was insufficient to enhance readmission detection and may require more sophisticated modelling strategies.

Therefore;

H1 rejected â†’ simple model outperformed RF

H2 supported â†’ utilisation features matter

H3 rejected â†’ oversampling hurt performance
"""

